---
# An instance of the Pages widget.
# Documentation: https://wowchemy.com/docs/page-builder/
widget: pages

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 90

title: 発表論文
subtitle: ''

---

[DBLP](http://dblp.uni-trier.de/pers/hd/y/Yonetani:Ryo) \| [Google Scholar](https://scholar.google.com/citations?user=DYXnRWEAAAAJ) \| [CiNii](http://ci.nii.ac.jp/nrid/9000017546008)

#### 査読付き雑誌論文
- Hiroaki Minoura, Ryo Yonetani, Mai Nishimura, Yoshitaka Ushiku: "Crowd Density Forecasting by Modeling Patch-based Dynamics", IEEE Robotics and Automation Letters **(RA-L)**, 2020.
- Ryo Yonetani, Kris Kitani, Yoichi Sato: "Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion Signatures", IEEE Transactions on Pattern Analysis and Machine Intelligence **(TPAMI)**, Vol.40, Issue 11, pp.2749-2761, 2018.
- 下西慶, 石川惠理奈, 米谷竜, 川嶋宏彰, 松山隆司: "視線運動解析による興味アスペクトの推定", ヒューマンインタフェース学会論文誌, Vol.16, No2, pp.103-114, 2014
- Akisato Kimura, Ryo Yonetani, Takatsugu Hirayama: "Computational Models of Human Visual Attention and Their Implementations: A Survey", IEICE Transactions on Information and Systems, E96-D(3), pp.562-578, 2013
- Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: Learning Spatiotemporal Gaps between Where We Look and What We Focus on", IPSJ Transactions on Computer Vision and Applications, 5, pp. 75-79, 2013
- 米谷竜, 川嶋宏彰, 加藤丈和, 松山隆司: "映像の顕著性変動モデルを用いた視聴者の集中状態推定, 電子情報通信学会論文誌, J96-D(8), pp.1675-1687, 2013（第15回 画像の認識・理解シンポジウム推薦論文）
- Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: "Mental Focus Analysis Using the Spatio-temporal Correlation between Visual Saliency and Eye Movements", 情報処理学会論文誌, Vol. 52, No. 12, 2011
- 石川惠理奈, 米谷竜, 平山高嗣, 松山隆司: "Gaze Mirroringによる注視模倣効果の分析", 情報処理学会論文誌, Vol.52, No.12, pp3637-3646, 2011
- 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "Gaze Probing: イベント提示に基づく注視オブジェクト推定", ヒューマンインタフェース学会論文誌, Vol.12, No.3, pp. 125-135, 2010

#### 国際会議論文
- Ryo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, Asako Kanezaki, "Path Planning using Neural A\* Search", International Conference on Machine Learning **(ICML)**, 2021 [[arXiv]](https://arxiv.org/abs/2009.07476) (accepted)
- Kazutoshi Tanaka, Ryo Yonetani, Masashi Hamaya, Robert Lee, Felix von Drigalski, Yoshihisa Ijiri, "TRANS-AM: Transfer Learning by Aggregating Dynamics Models for Soft Robotic Assembly", International Conference on Robotics and Automation **(ICRA)**, 2021 (accepted)
- Felix von Drigalski, Masashi Hayashi, Yifei Huang, Ryo Yonetani, Masashi Hamaya, Kazutoshi Tanaka, and Yoshihisa Ijiri, "Precise Multi-Modal In-Hand Pose Estimation using Low-Precision Sensors for Robotic Assembly", International Conference on Robotics and Automation **(ICRA)**, 2021 (accepted)
- Jiaxin Ma, Ryo Yonetani, Zahid Iqbal, "Adaptive Distillation for Decentralized Learning from Heterogeneous Clients", International Conference on Pattern Recognition **(ICPR)**, 2020 [[arXiv]](https://arxiv.org/abs/2008.07948)
- Mai Nishimura, Ryo Yonetani, "L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation", accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems **(IROS)**, 2020, [[arXiv]](https://arxiv.org/abs/2003.09207)
- Mohammadamin Barekatain, Ryo Yonetani, Masashi Hamaya, "MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics", International Joint Conference on Artificial Intelligence **(IJCAI)**, 2020 [[arXiv](https://arxiv.org/abs/1909.13111)]
- Rie Kamikubo, Naoya Kato, Keita Higuchi, Ryo Yonetani, Yoichi Sato, "Studying Effective Agents in Remote Sighted Guidance for People Navigating with Visual Impairments", ACM Conference on Human Factors in Computing Systems **(CHI)**, 2020 
- Navyata Sanghvi, Ryo Yonetani, Kris Kitani, "Modeling Social Group Communication with Multi-Agent Imitation Learning", International Conference on Autonomous Agents and Multi-Agent Systems **(AAMAS)**, 2020 [[arXiv]](https://arxiv.org/abs/1903.01537)
- Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto, Ryo Yonetani, "Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using Non-IID Data", IEEE International Conference on Communications **(ICC)**, 2020 [[arXiv]](https://arxiv.org/abs/1905.07210)
- Takayuki Nishio and Ryo Yonetani: "Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge", IEEE International Conference on Communications **(ICC)**, 2019
- Nathawan Charoenkulvanich, Rie Kamikubo, Ryo Yonetani, and Yoichi Sato, "Assisting Group Activity Analysis through Hand Detection and Identification in Multiple Egocentric Videos", ACM Conference on Intelligent User Interface **(IUI)**, 2019.
- Yuki Sugita, Keita Higuchi, Ryo Yonetani, Rie Kamikubo, Yoichi Sato: "Browsing Group First-Person Videos with 3D Visualization", accepted to ACM International Conference on Interactive Surfaces and Spaces (ISS), 2018
- Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, Yoichi Sato: "Future Person Localization in First-Person Videos", IEEE Conference on Computer Vision and Pattern Recognition **(CVPR, spotlight presentation)**, 2018
- Rie Kamikubo, Keita Higuchi, Ryo Yonetani, Hideki Koike, Yoichi Sato, "Exploring the Role of Tunnel Vision Simulation in the Design Cycle of Accessible Interfaces", International Cross-Disciplinary Conference on Web Accessibility **(Web4All)**, 2018
- Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, Yoichi Sato: "Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption", International Conference on Computer Vision **(ICCV)**, 2017
- Keita Higuchi, Ryo Yonetani, Yoichi Sato: "EgoScanning: Quickly Scanning First-Person Videos with Egocentric Elastic Timelines", ACM Conference on Human Factors in Computing Systems **(CHI)**, 2017
- Ryo Yonetani, Kris Kitani, Yoichi Sato: "Visual Motif Discovery via First-Person Vision", European Conference on Computer Vision **(ECCV)**, 2016
- Ryo Yonetani, Kris Kitani, Yoichi Sato: "Recognizing Micro-Actions and Reactions from Paired Egocentric Videos", IEEE Conference on Computer Vision and Pattern Recognition **(CVPR)**, 2016
- Keita Higuchi, Ryo Yonetani, Yoichi Sato: "Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks", ACM Conference on Human Factors in Computing Systems **(CHI)**, 2016
- Ryo Yonetani, Kris Kitani, Yoichi Sato: "Ego-Surfing First-Person Videos", IEEE Conference on Computer Vision and Pattern Recognition **(CVPR)**, 2015
- Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: "Predicting Where We Look from Spatiotemporal Gaps", International Conference on Multimodal Interaction **(ICMI)**, 2013
- Ryo Yonetani, Akisato Kimura, Hitoshi Sakano, Ken Fukuchi: "Single Image Segmentation with Estimated Depth", British Machine Vision Conference **(BMVC)**, 2012
- Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: "Multi-mode Saliency Dynamics Model for Analyzing Gaze and Attention", Eye Tracking Research & Applications **(ETRA)**, 2012
- Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: "Gaze Probing: Event-Based Estimation of Objects Being Focused On", International Conference on Pattern Recognition **(ICPR, IBM Best Student Paper Award)**, 2010

#### 国際ワークショップ論文，Extended Abstracts, プレプリントなど
- Ryo Yonetani, Tomohiro Takahashi, Atsushi Hashimoto, Yoshitaka Ushiku, "Decentralized Learning of Generative Adversarial Networks from Multi-Client Non-iid Data", arXiv preprint, 2019 [[arXiv]](https://arxiv.org/abs/1905.09684)
- Navyata Sanghvi, Ryo Yonetani, Kris Kitani, "Learning Group Communication from Demonstration", RSS Workshop on Models and Representations for Natural Human-Robot Communication, 2018
- Seita Kayukawa, Keita Higuchi, Ryo Yonetani, Maanori Nakamura, Yoichi Sato, Shigeo Morishima: "Dynamic Object Scanning: Object-Based Elastic Timeline for Quickly Browsing First-Person Videos", ACM Conference on Human Factors in Computing Systems Late Breaking Work **(CHI-LBW)**, 2018
- Keita Higuchi, Ryo Yonetani, Yoichi Sato: "EgoScanning: Quickly Scanning First-Person Videos with Egocentric Elastic Timelines", ACM SIGGRAPH Asia Emerging Technologies **(SIGGRAPH-ASIA-ETECH)**, 2017
- Yifei Huang, Minjie Cai, Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: "Temporal Localization and Spatial Segmentation of Joint Attention in Multiple First-Person Video", International Workshop on Egocentric Perception, Interaction, and Computing **(EPIC)**, 2017
- Rie Kamikubo, Keita Higuchi, Ryo Yonetani, Hideki Koike, Yoichi Sato: "Rapid Prototyping of Accessible Interfaces with Gaze- Contiguent Tunnel Vision Simulation", ACM SIGACCESS International Conference on Computers and Accessibility **(ASSETS)**, 2017
- Ryo Yonetani, Vishnu Naresh Boddeti, Kris Kitani, Yoichi Sato: "Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption", International Workshop on The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security **(CV-COPS)**, 2017
- Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: "Discovering Objects of Joint Attention via First-Person Sensing", IEEE CVPR Workshop on Egocentric (First-Person) Vision **(EGOV)**, 2016
- Kei Shimonishi, Hiroaki Kawashima, Ryo Yonetani, Erina Ishikawa, Takashi Matsuyama: "Learning Aspects of Interest from Gaze", ICMI Workshop on Eye Gaze in Intelligent Human Machine Interaction: Gaze in Multimodal Interaction **(GazeIn)**, 2013
- Ryo Yonetani: "Modeling Video Viewing Behaviors for Viewer State Estimation", ACM Multimedia Doctoral Symposium **(ACMMM-DS)**, 2012
- Erina Ishikawa, Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: "Semantic Interpretation of Eye Movements Using Designed Structures of Displayed Contents", ICMI Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye Gaze, Multimodality **(GazeIn)**, 2012

#### 国内会議論文
- 八木 拓真, マンガラムカーティケヤ, 米谷 竜, 佐藤 洋一, "一人称視点映像における人物位置予測", 情報処理学会研究会資料 CVIM, 2018
- 粥川青汰, 樋口啓太, 中村優文, 米谷竜, 佐藤洋一, 森島繁生, ”一人称視点動画の高速閲覧に有効なキューの自動生成手法”, インタラクティブシステムとソフトウェアに関するワークショップ,  2017
- 粥川青汰, 樋口啓太, 中村優文, 米谷竜, 佐藤洋一, 森島繁生, ”物体検出とユーザ入力に基づく一人称視点映像の高速閲覧手法”, 情報処理学会コンピュータビジョンとイメージメディア研究会,  2017.
- Yifei Huang, Minjie Cai, Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: "Spatial-temporal Segmentation of Joint Attention in Multiple First-Person Videos", 第20回 画像の認識理解シンポジウム, 2017
- 杉田祐樹, 樋口啓太, 米谷竜, 佐藤洋一: "複数一人称視点映像閲覧における行動空間とカメラ位置姿勢の3次元可視化による効果", 情報処理学会シンポジウム　インタラクション2017, 2017**（インタラクティブ発表賞）**
- 中野雄介, 米谷竜, 樋口啓太, 佐藤洋一: "視線を考慮した一人称視点映像からの頷き検出", 電子情報通信学会総合大会, 2017
- 杉田祐樹, 樋口啓太, 米谷竜, 佐藤洋一: "複数一人称視点映像閲覧における行動空間とカメラ位置姿勢の3次元可視化による効果", 情報処理学会ヒューマンコンピュータインタラクション研究会, 2017
- 樋口啓太, 米谷竜, 佐藤洋一: "伸縮タイムライン生成による一人称視点映像の高速閲覧支援", 第24回インタラクティブシステムとソフトウェアに関するワークショップ, 2016
- 樋口未来, 米谷竜, 木谷クリス, 佐藤洋一: "一人称視点映像を用いたランキング学習による相対的地位の推定", 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016**（情報処理学会CVIM研究会奨励賞）**
- Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: "Discovering Objects of Joint Attention via First-Person Sensing", 第19回 画像の認識理解シンポジウム,2016
- 樋口啓太, 米谷竜, 佐藤洋一: "手の動作に基づく複数一人称視点作業映像のアラインメント", 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016
- 松本大輝, 米谷竜, 佐藤洋一: "滑動性眼球運動を用いた視線計測の自動校正", 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016
- 村上晋太郎, 米谷竜, 佐藤洋一: "視線を利用した二人称視点動作認識", 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016**（情報処理学会CVIM研究会奨励賞）**
- Ryo Yonetani, Kris Kitani, Yoichi Sato: "Ego-Surfing First-Person Videos", 第18回 画像の認識理解シンポジウム, 2015
- 松本 大輝, 米谷 竜, 佐藤 洋一, "滑動性眼球運動を用いた視線計測の自動校正", 第18回 画像の認識理解シンポジウム, 2015
- 杉田 祐樹, 米谷 竜, 佐藤 洋一, "一人称視点映像における視線情報を活用した自己アクティビティ認識", 第18回 画像の認識理解シンポジウム, 2015
- 樋口 未来, 木谷 クリス 真実, 米谷 竜, 佐藤 洋一, "一人称視点映像を用いた話者間の相対的地位の推定", 第18回 画像の認識理解シンポジウム, 2015
- 樋口啓太, 米谷竜, 佐藤洋一: "遠隔作業支援シナリオにおける注視位置可視化の効果", 第23回インタラクティブシステムとソフトウェアに関するワークショップ, 2015
- 神窪利絵, 樋口啓太, 米谷竜, 小池英樹, 佐藤洋一: "弱視者のための視線計測を用いたウェブアクセシビリティの向上", 電子情報通信学会福祉情報工学研究会, 信学技報WIT2015-75, 2015
- Ryo Yonetani, Hiroaki Kawashima, and Takashi Matsuyama: Modeling Spatiotemporal Correlations between Video Saliency and Gaze Dynamics", 研究報告コンピュータビジョンとイメージメディア, 2014-CVIM-192(32), pp. 1-16, 2014
- 米谷竜, 川嶋宏彰, 松山隆司: "映像閲覧行動の時空間ずれ構造モデルを用いた注視点予測", 信学技報, vol. 113, no. 196, PRMU2013-41, pp. 57-62, 2013**（2013年度PRMU研究奨励賞）**
- Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: "Learning Spatiotemporal Gaps between Where We Look and What We Focus on", 第16回 画像の認識理解シンポジウム, 2013
- 下西慶, 川嶋宏彰, 米谷竜, 松山隆司: "視線運動解析による興味アスペクトの推定", 信学技報, vol. 113, no. 75, PRMU2013-28, pp. 53-58, 2013
- 石川惠理奈, 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "提示コンテンツのデザイン構造を用いた視線運動の意味理解", 電子情報通信学会技術報告, PRMU2012-60, vol. 112, no. 225, pp.47-52, 2012
- 米谷竜, 川嶋宏彰, 加藤丈和, 松山隆司: "映像の顕著性変動モデルを用いた視聴者の集中状態推定", 第15回 画像の認識・理解シンポジウム, 2012**（MIRU優秀学生論文賞）**
- 石川惠理奈, 米谷竜, 平山高嗣, 松山隆司: "Gaze Mirroringによる注視模倣効果の分析", ヒューマンインタフェースシンポジウム2011, pp.561-566, 2011
- 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "映像の顕著性変動と視線運動の時空間相関分析に基づいた集中状態推定", 情報処理学会研究会資料, CVIM178-16, 2011
- 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "注視オブジェクト推定のための動的コンテンツデザインとその評価", 情報処理学会創立50周年記念（第72回）全国大会, 32N-1, pp. 5-141-142, 2010
- 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "Gaze Probing, "イベント提示に基づく注視対象推定", 第12回画像の認識・理解シンポジウム, pp.1713-1720, 2009
- 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: "提示イベントと眼球動作との同期構造分析に基づく注視対象推定", 情報処理学会研究会資料 CVIM 167-16, 2009

#### 特許
https://patents.google.com/?inventor=Ryo+Yonetani 